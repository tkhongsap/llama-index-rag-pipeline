#!/usr/bin/env python3
"""
Test script for BGE embedding integration with iLand docs_embedding module.
Tests both Hugging Face BGE model and OpenAI embeddings for compatibility.
"""

import os
import sys
import time
import numpy as np
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv

# Add src-iLand to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src-iLand"))

# Load environment variables
load_dotenv()

def test_installations():
    """Test if required packages are installed."""
    print("ğŸ” Testing package installations...")
    
    required_packages = [
        "llama_index",
        "sentence_transformers", 
        "transformers",
        "torch"
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
            print(f"  âœ… {package} installed")
        except ImportError:
            print(f"  âŒ {package} missing")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nğŸ“¦ Install missing packages:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    return True

def test_bge_embedding_basic():
    """Test basic BGE embedding functionality."""
    print("\nğŸ§ª Testing BGE embedding basic functionality...")
    
    try:
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        
        # Initialize BGE embedding model
        embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-small-en-v1.5",
            cache_folder="./cache"
        )
        
        # Test embedding generation
        test_text = "This is a test document about Thai land deeds and property information."
        
        print(f"  ğŸ“ Test text: {test_text[:50]}...")
        
        start_time = time.time()
        embedding = embed_model.get_text_embedding(test_text)
        duration = time.time() - start_time
        
        print(f"  âœ… BGE embedding successful")
        print(f"  ğŸ“Š Embedding dimension: {len(embedding)}")
        print(f"  â±ï¸ Generation time: {duration:.3f}s")
        print(f"  ğŸ“ˆ Sample values: {embedding[:5]}")
        
        return embed_model, embedding
        
    except Exception as e:
        print(f"  âŒ BGE embedding failed: {str(e)}")
        return None, None

def test_openai_embedding_basic():
    """Test OpenAI embedding for comparison."""
    print("\nğŸ§ª Testing OpenAI embedding basic functionality...")
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("  âš ï¸ OPENAI_API_KEY not found - skipping OpenAI test")
        return None, None
    
    try:
        from llama_index.embeddings.openai import OpenAIEmbedding
        
        # Initialize OpenAI embedding model
        embed_model = OpenAIEmbedding(
            model="text-embedding-3-small",
            api_key=api_key
        )
        
        # Test embedding generation
        test_text = "This is a test document about Thai land deeds and property information."
        
        print(f"  ğŸ“ Test text: {test_text[:50]}...")
        
        start_time = time.time()
        embedding = embed_model.get_text_embedding(test_text)
        duration = time.time() - start_time
        
        print(f"  âœ… OpenAI embedding successful")
        print(f"  ğŸ“Š Embedding dimension: {len(embedding)}")
        print(f"  â±ï¸ Generation time: {duration:.3f}s")
        print(f"  ğŸ“ˆ Sample values: {embedding[:5]}")
        
        return embed_model, embedding
        
    except Exception as e:
        print(f"  âŒ OpenAI embedding failed: {str(e)}")
        return None, None

def test_embedding_compatibility(bge_embedding, openai_embedding):
    """Test compatibility between BGE and OpenAI embeddings."""
    print("\nğŸ”„ Testing embedding compatibility...")
    
    if bge_embedding is None or openai_embedding is None:
        print("  âš ï¸ Cannot test compatibility - one or both embeddings missing")
        return False
    
    # Check dimensions
    bge_dim = len(bge_embedding)
    openai_dim = len(openai_embedding)
    
    print(f"  ğŸ“Š BGE dimension: {bge_dim}")
    print(f"  ğŸ“Š OpenAI dimension: {openai_dim}")
    
    if bge_dim != openai_dim:
        print(f"  âš ï¸ Different dimensions: BGE={bge_dim}, OpenAI={openai_dim}")
        print("  ğŸ’¡ This is expected - different models have different dimensions")
    
    # Test similarity calculation
    try:
        # Convert to numpy arrays for calculation
        bge_arr = np.array(bge_embedding)
        openai_arr = np.array(openai_embedding)
        
        # Normalize vectors
        bge_norm = bge_arr / np.linalg.norm(bge_arr)
        openai_norm = openai_arr / np.linalg.norm(openai_arr)
        
        print(f"  ğŸ“Š BGE vector norm: {np.linalg.norm(bge_arr):.4f}")
        print(f"  ğŸ“Š OpenAI vector norm: {np.linalg.norm(openai_arr):.4f}")
        
        print("  âœ… Both embeddings are compatible for vector operations")
        return True
        
    except Exception as e:
        print(f"  âŒ Compatibility test failed: {str(e)}")
        return False

def test_docs_embedding_with_bge():
    """Test the docs_embedding module with BGE embeddings."""
    print("\nğŸ—ï¸ Testing docs_embedding module with BGE...")
    
    try:
        # Import docs_embedding modules
        from docs_embedding.document_loader import iLandDocumentLoader
        from docs_embedding.metadata_extractor import iLandMetadataExtractor
        from docs_embedding.embedding_processor import EmbeddingProcessor
        
        # Check if we have sample documents
        example_dir = Path("example")
        if not example_dir.exists():
            print("  âš ï¸ No example directory found - creating minimal test")
            return test_minimal_integration()
        
        # Find sample documents
        sample_files = list(example_dir.rglob("*.md"))
        if not sample_files:
            print("  âš ï¸ No markdown files found in example directory")
            return test_minimal_integration()
        
        # Take first sample file
        sample_file = sample_files[0]
        print(f"  ğŸ“„ Testing with: {sample_file.name}")
        
        # Load document
        loader = iLandDocumentLoader()
        docs = loader.load_documents_from_files([sample_file])
        
        if not docs:
            print("  âŒ Failed to load document")
            return False
        
        print(f"  âœ… Loaded document: {docs[0].metadata.get('title', 'Unknown')}")
        print(f"  ğŸ“Š Document length: {len(docs[0].text)} characters")
        
        # Test with BGE embedding
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        
        bge_embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-small-en-v1.5",
            cache_folder="./cache"
        )
        
        # Test embedding a chunk of the document
        test_chunk = docs[0].text[:500]  # First 500 characters
        
        start_time = time.time()
        embedding = bge_embed_model.get_text_embedding(test_chunk)
        duration = time.time() - start_time
        
        print(f"  âœ… BGE embedding successful for real document")
        print(f"  ğŸ“Š Embedding dimension: {len(embedding)}")
        print(f"  â±ï¸ Processing time: {duration:.3f}s")
        
        return True
        
    except Exception as e:
        print(f"  âŒ docs_embedding BGE test failed: {str(e)}")
        return False

def test_minimal_integration():
    """Test minimal integration without sample documents."""
    print("  ğŸ”§ Running minimal integration test...")
    
    try:
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        from llama_index.core import Document
        
        # Create minimal test document
        test_doc = Document(
            text="à¹‚à¸‰à¸™à¸”à¸—à¸µà¹ˆà¸”à¸´à¸™ à¹€à¸¥à¸‚à¸—à¸µà¹ˆ 12345 à¸ˆà¸±à¸‡à¸«à¸§à¸±à¸” à¸à¸£à¸¸à¸‡à¹€à¸—à¸à¸¡à¸«à¸²à¸™à¸„à¸£ à¹€à¸‚à¸• à¸šà¸²à¸‡à¸à¸°à¸›à¸´",
            metadata={
                "deed_type": "Chanote",
                "province": "Bangkok", 
                "district": "Bang Kapi",
                "deed_serial_no": "12345"
            }
        )
        
        # Test BGE embedding
        bge_embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-small-en-v1.5",
            cache_folder="./cache"
        )
        
        embedding = bge_embed_model.get_text_embedding(test_doc.text)
        
        print(f"  âœ… Minimal integration successful")
        print(f"  ğŸ“Š Embedding dimension: {len(embedding)}")
        print(f"  ğŸ“‹ Test metadata: {test_doc.metadata}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Minimal integration failed: {str(e)}")
        return False

def test_performance_comparison():
    """Compare performance between BGE and OpenAI embeddings."""
    print("\nâš¡ Testing performance comparison...")
    
    test_texts = [
        "à¹‚à¸‰à¸™à¸”à¸—à¸µà¹ˆà¸”à¸´à¸™ à¹€à¸¥à¸‚à¸—à¸µà¹ˆ 12345 à¸ˆà¸±à¸‡à¸«à¸§à¸±à¸” à¸à¸£à¸¸à¸‡à¹€à¸—à¸à¸¡à¸«à¸²à¸™à¸„à¸£",
        "Land deed document with property information and measurements",
        "à¸—à¸µà¹ˆà¸”à¸´à¸™ à¸‚à¸™à¸²à¸” 2 à¹„à¸£à¹ˆ 3 à¸‡à¸²à¸™ 45 à¸•à¸²à¸£à¸²à¸‡à¸§à¸² à¹ƒà¸™à¹€à¸‚à¸• à¸šà¸²à¸‡à¸à¸°à¸›à¸´ à¸à¸£à¸¸à¸‡à¹€à¸—à¸à¸¯",
        "Property location coordinates and boundary descriptions",
        "à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸—à¸µà¹ˆà¸”à¸´à¸™ à¹à¸¥à¸°à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ˆà¹‰à¸²à¸‚à¸­à¸‡"
    ]
    
    results = {}
    
    # Test BGE performance
    try:
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        
        bge_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-small-en-v1.5",
            cache_folder="./cache"
        )
        
        print("  ğŸ”„ Testing BGE performance...")
        start_time = time.time()
        
        bge_embeddings = []
        for text in test_texts:
            embedding = bge_model.get_text_embedding(text)
            bge_embeddings.append(embedding)
        
        bge_duration = time.time() - start_time
        results["bge"] = {
            "duration": bge_duration,
            "texts_processed": len(test_texts),
            "avg_time": bge_duration / len(test_texts),
            "dimension": len(bge_embeddings[0])
        }
        
        print(f"  âœ… BGE: {len(test_texts)} texts in {bge_duration:.3f}s")
        print(f"      Average: {bge_duration/len(test_texts):.3f}s per text")
        
    except Exception as e:
        print(f"  âŒ BGE performance test failed: {str(e)}")
    
    # Test OpenAI performance (if API key available)
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        try:
            from llama_index.embeddings.openai import OpenAIEmbedding
            
            openai_model = OpenAIEmbedding(
                model="text-embedding-3-small",
                api_key=api_key
            )
            
            print("  ğŸ”„ Testing OpenAI performance...")
            start_time = time.time()
            
            openai_embeddings = []
            for text in test_texts:
                embedding = openai_model.get_text_embedding(text)
                openai_embeddings.append(embedding)
            
            openai_duration = time.time() - start_time
            results["openai"] = {
                "duration": openai_duration,
                "texts_processed": len(test_texts),
                "avg_time": openai_duration / len(test_texts),
                "dimension": len(openai_embeddings[0])
            }
            
            print(f"  âœ… OpenAI: {len(test_texts)} texts in {openai_duration:.3f}s")
            print(f"      Average: {openai_duration/len(test_texts):.3f}s per text")
            
        except Exception as e:
            print(f"  âŒ OpenAI performance test failed: {str(e)}")
    
    # Compare results
    if len(results) > 1:
        print("\n  ğŸ“Š Performance Summary:")
        for model_name, metrics in results.items():
            print(f"    {model_name.upper()}:")
            print(f"      - Total time: {metrics['duration']:.3f}s")
            print(f"      - Avg per text: {metrics['avg_time']:.3f}s")
            print(f"      - Dimension: {metrics['dimension']}")
    
    return results

def main():
    """Main test runner."""
    print("ğŸš€ BGE EMBEDDING INTEGRATION TEST")
    print("=" * 50)
    
    # Test 1: Package installations
    if not test_installations():
        print("\nâŒ Package installation check failed")
        print("Please install missing packages and retry")
        return False
    
    # Test 2: Basic BGE embedding
    bge_model, bge_embedding = test_bge_embedding_basic()
    
    # Test 3: Basic OpenAI embedding (for comparison)
    openai_model, openai_embedding = test_openai_embedding_basic()
    
    # Test 4: Embedding compatibility
    test_embedding_compatibility(bge_embedding, openai_embedding)
    
    # Test 5: Integration with docs_embedding module
    integration_success = test_docs_embedding_with_bge()
    
    # Test 6: Performance comparison
    performance_results = test_performance_comparison()
    
    # Final summary
    print("\n" + "=" * 50)
    print("ğŸ“‹ TEST SUMMARY")
    print("=" * 50)
    
    tests_results = [
        ("Package Installation", test_installations()),
        ("BGE Basic Functionality", bge_embedding is not None),
        ("OpenAI Basic Functionality", openai_embedding is not None),
        ("Embedding Compatibility", bge_embedding is not None and openai_embedding is not None),
        ("docs_embedding Integration", integration_success),
        ("Performance Testing", len(performance_results) > 0)
    ]
    
    passed = sum(1 for _, result in tests_results if result)
    total = len(tests_results)
    
    for test_name, result in tests_results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {status} {test_name}")
    
    print(f"\nğŸ“Š Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed! BGE embedding is ready for use.")
    else:
        print("âš ï¸ Some tests failed. Check the output above for details.")
    
    print("\nğŸ’¡ Next Steps:")
    print("  1. Update requirements.txt to include BGE dependencies")
    print("  2. Modify embedding_processor.py to support BGE models")
    print("  3. Add configuration option to switch between OpenAI and BGE")
    print("  4. Test with full document processing pipeline")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)