#!/usr/bin/env python
"""
iLand BGE-M3 Enhanced PostgreSQL RAG Pipeline

This script provides a unified workflow implementing PRD v2.0 requirements:
1. Processing Excel/CSV files to PostgreSQL markdown documents with rich metadata
2. Generating embeddings using section-based chunking with BGE-M3 + OpenAI fallback
3. Storing embeddings in PostgreSQL with systematic multi-table structure

Features (PRD v2.0 Implementation):
- BGE-M3 multilingual model for Thai language with OpenAI fallback
- Section-based chunking reducing chunks from ~169 to ~6 per document
- Complete metadata preservation and enhancement (30+ fields)
- pgVector storage structure maintained (single table, no schema changes)
- Production-ready error handling and comprehensive logging
- Local BGE processing priority with cloud fallback option

Usage:
    # Basic usage with section-based chunking and BGE-M3 + OpenAI fallback
    python bge_postgres_pipeline.py
    
    # Process limited documents for testing
    python bge_postgres_pipeline.py --max-rows 100 --skip-processing
    
    # BGE-only processing (no OpenAI fallback)
    python bge_postgres_pipeline.py --disable-multi-model
    
    # Traditional sentence chunking (not recommended for PRD compliance)
    python bge_postgres_pipeline.py --disable-section-chunking
"""

import os
import sys
import argparse
import logging
from pathlib import Path
import time
from typing import Tuple
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Ensure src-iLand is in the Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# Import required modules
try:
    from data_processing_postgres.iland_converter import iLandCSVConverter
    from docs_embedding_postgres.postgres_embedding_bge import BGEPostgresEmbeddingGenerator
except ImportError as e:
    logger.error(f"Import error: {e}")
    logger.error("Required modules for BGE PostgreSQL pipeline not found.")
    logger.error("Make sure you're running this script from the project root directory.")
    logger.error("Expected modules:")
    logger.error("  - data_processing_postgres.iland_converter")
    logger.error("  - docs_embedding_postgres.postgres_embedding_bge")
    sys.exit(1)


def process_data(args):
    """Process data from Excel/CSV to PostgreSQL markdown documents"""
    logger.info("=== STEP 1: DATA PROCESSING ===")
    
    # Auto-detect paths
    project_root = Path(__file__).parent.parent
    input_dir = project_root / "data" / "input_docs"
    
    # Look for the iLand data file
    input_filename = args.input_file if args.input_file else "input_dataset_iLand.xlsx"
    input_file = input_dir / input_filename
    
    if not input_file.exists():
        raise FileNotFoundError(
            f"Could not find iLand data file: {input_file}\n"
            f"Please ensure {input_filename} exists in data/input_docs/"
        )
    
    # Set output directory
    output_dir = str(project_root / "data" / "output_docs")
    
    logger.info(f"Using iLand data file: {input_file}")
    logger.info(f"Output directory: {output_dir}")
    logger.info(f"Max rows to process: {args.max_rows or 'All'}")
    logger.info(f"Database host: {args.db_host}, port: {args.db_port}")
    
    if args.filter_province:
        logger.info(f"Filtering data for province: {args.filter_province}")
    
    # Create iLand converter
    converter = iLandCSVConverter(str(input_file), output_dir)
    
    # Set database connection parameters
    converter.db_manager.db_host = args.db_host
    converter.db_manager.db_port = args.db_port
    converter.db_manager.db_name = args.db_name
    converter.db_manager.db_user = args.db_user
    converter.db_manager.db_password = args.db_password
    
    # Setup configuration
    config = converter.setup_configuration(config_name="iland_deed_records", auto_generate=True)
    
    # Process documents
    logger.info("Processing dataset in batches...")
    documents = converter.process_csv_to_documents(
        batch_size=args.batch_size, 
        max_rows=args.max_rows,
        filter_province=args.filter_province
    )
    
    # Save documents as JSONL for backup
    jsonl_path = converter.save_documents_as_jsonl(documents)
    
    # Save documents to PostgreSQL database
    logger.info("Saving documents to PostgreSQL database...")
    inserted_count = converter.save_documents_to_database(
        documents, 
        batch_size=args.db_batch_size
    )
    
    # Print summary statistics
    converter.print_summary_statistics(documents)
    
    logger.info("iLand dataset conversion completed successfully!")
    logger.info(f"Total documents created: {len(documents)}")
    logger.info(f"JSONL output: {jsonl_path}")
    logger.info(f"Database insertion: {inserted_count} documents inserted into {args.source_table} table")
    
    return inserted_count


def generate_embeddings(args, document_count) -> bool:
    """Generate BGE-M3 embeddings with section-based chunking and store in pgVector"""
    logger.info("\n=== STEP 2: BGE-M3 EMBEDDING GENERATION WITH SECTION-BASED CHUNKING ===")
    logger.info("ðŸ”’ Using BGE-M3 with OpenAI fallback - Priority for local processing")
    logger.info("ðŸ“ Section-based chunking: ~6 chunks per document vs ~169 traditional")
    logger.info("ðŸ—„ï¸ Storage: pgVector (single table structure)")
    
    # Create BGE PostgreSQL embedding generator with section-based chunking
    embedding_generator = BGEPostgresEmbeddingGenerator(
        # Source database (where markdown content is stored)
        source_db_name=args.db_name,
        source_db_user=args.db_user,
        source_db_password=args.db_password,
        source_db_host=args.db_host,
        source_db_port=args.db_port,
        source_table_name=args.source_table,
        
        # Destination database (for embeddings) - pgVector table
        dest_db_name=args.db_name,
        dest_db_user=args.db_user,
        dest_db_password=args.db_password,
        dest_db_host=args.db_host,
        dest_db_port=args.db_port,
        dest_table_name=args.embeddings_table,
        
        # BGE-M3 model configuration (as per PRD specifications)
        bge_model_key=args.bge_model,  # Default to "bge-m3" for multilingual Thai support
        cache_folder=args.cache_folder,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        batch_size=args.embed_batch_size,
        
        # Enhanced processing settings for section-based chunking
        enable_section_chunking=not args.disable_section_chunking,  # Enable section-based parsing per PRD
        min_section_size=args.min_section_size,
        
        # Multi-model fallback settings
        fallback_to_openai=not args.disable_multi_model  # Enable OpenAI fallback unless disabled
    )
    
    try:
        # Connect to source database
        if not embedding_generator.connect_to_source_db():
            logger.error("Failed to connect to source database. Aborting.")
            return False
        
        logger.info(f"âœ… Connected to source database: {args.db_name}")
        logger.info(f"ðŸ“Š BGE Model: {args.bge_model} ({embedding_generator.embed_dim}d)")
        logger.info(f"ðŸ”§ Section-based chunking: {'Enabled' if not args.disable_section_chunking else 'Disabled'}")
        logger.info(f"ðŸ”’ OpenAI fallback: {'Enabled' if not args.disable_multi_model else 'Disabled'}")
        logger.info(f"ðŸ—„ï¸ pgVector table: {args.embeddings_table}")
        
        # Run the BGE embedding generation
        logger.info(f"ðŸ“„ Starting BGE processing for {document_count or 'all'} documents...")
        
        inserted_count = embedding_generator.run_pipeline(limit=document_count)
        
        if inserted_count > 0:
            # Get processing statistics
            stats = embedding_generator.processing_stats
            
            logger.info("ðŸŽ‰ SUCCESS! BGE-M3 Section-Based Embeddings Generated Successfully")
            logger.info("=" * 60)
            logger.info(f"ðŸ“ˆ PROCESSING STATISTICS:")
            logger.info(f"  - Documents processed: {stats['documents_processed']}")
            logger.info(f"  - Nodes created: {stats['nodes_created']}")
            logger.info(f"  - Section chunks: {stats['section_chunks']}")
            logger.info(f"  - Fallback chunks: {stats['fallback_chunks']}")
            logger.info(f"  - Embeddings inserted: {inserted_count}")
            logger.info(f"  - Metadata fields extracted: {stats['metadata_fields_extracted']}")
            logger.info(f"  - Embedding provider: {stats['embedding_provider']}")
            logger.info(f"  - Model used: {stats['model_name']}")
            logger.info("=" * 60)
            
            # Calculate average chunks per document (section-based should be much lower)
            if stats["documents_processed"] > 0:
                avg_chunks_per_doc = stats["nodes_created"] / stats["documents_processed"]
                avg_meta_per_doc = stats["metadata_fields_extracted"] / stats["documents_processed"]
                
                logger.info(f"  - Average nodes per doc: {avg_chunks_per_doc:.1f}")
                logger.info(f"  - Average metadata fields per doc: {avg_meta_per_doc:.1f}")
                
                # Verify section-based chunking success (should be much less than 169)
                if avg_chunks_per_doc < 20:  # Much better than 169 chunks
                    logger.info("âœ… Section-based chunking SUCCESS: Efficient chunk distribution achieved")
                else:
                    logger.warning(f"âš ï¸ Section-based chunking may need optimization: {avg_chunks_per_doc:.1f} chunks per doc")
            
            # Verify BGE model usage
            if stats["embedding_provider"] == "bge":
                logger.info("ðŸ”’ Security compliance: BGE local processing activated")
            else:
                logger.info("ðŸ”‘ Using OpenAI fallback - BGE may not be available")
            
            return True
        else:
            logger.error("Embedding generation failed or no embeddings were created")
            return False
            
    except Exception as e:
        logger.error(f"Error in embedding generation: {e}")
        logger.error("This may indicate issues with BGE model initialization or database connectivity")
        return False
    finally:
        # Close database connection
        embedding_generator.close_source_db()


def main():
    """Main execution function"""
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description='iLand BGE-M3 RAG Pipeline with Section-Based Chunking (PRD v2.0 - pgVector Storage)'
    )
    
    # Data processing arguments
    parser.add_argument('--max-rows', type=int, default=None,
                        help='Maximum number of rows to process (default: all)')
    parser.add_argument('--batch-size', type=int, default=500,
                        help='Batch size for processing (default: 500)')
    parser.add_argument('--db-batch-size', type=int, default=100,
                        help='Batch size for database insertion (default: 100)')
    parser.add_argument('--db-host', type=str, default=os.getenv("DB_HOST"),
                        help=f'Database host (default: {os.getenv("DB_HOST")})')
    parser.add_argument('--db-port', type=int, default=int(os.getenv("DB_PORT", "5432")),
                        help=f'Database port (default: {os.getenv("DB_PORT", "5432")})')
    parser.add_argument('--db-name', type=str, default=os.getenv("DB_NAME", "iland-vector-dev"),
                        help=f'Database name (default: {os.getenv("DB_NAME", "iland-vector-dev")})')
    parser.add_argument('--db-user', type=str, default=os.getenv("DB_USER", "vector_user_dev"),
                        help=f'Database user (default: {os.getenv("DB_USER", "vector_user_dev")})')
    parser.add_argument('--db-password', type=str, default=os.getenv("DB_PASSWORD"),
                        help='Database password (default: from .env)')
    parser.add_argument('--input-file', type=str, default=None,
                        help='Custom input filename (default: input_dataset_iLand.xlsx)')
    parser.add_argument('--source-table', type=str, default=os.getenv("SOURCE_TABLE", "iland_md_data"),
                        help=f'Source table name (default: {os.getenv("SOURCE_TABLE", "iland_md_data")})')
    parser.add_argument('--filter-province', type=str, default="à¸Šà¸±à¸¢à¸™à¸²à¸—",
                        help='Filter data by province name (default: "à¸Šà¸±à¸¢à¸™à¸²à¸—")')
    
    # BGE-M3 model arguments (enhanced as per PRD)
    parser.add_argument('--bge-model', type=str, default=os.getenv("BGE_MODEL", "bge-m3"),
                        help=f'BGE model name (default: {os.getenv("BGE_MODEL", "bge-m3")} - multilingual for Thai)')
    parser.add_argument('--cache-folder', type=str, default=os.getenv("CACHE_FOLDER", "./cache/bge_models"),
                        help=f'BGE model cache folder (default: {os.getenv("CACHE_FOLDER", "./cache/bge_models")})')
    
    # Enhanced embedding arguments for section-based chunking
    parser.add_argument('--chunk-size', type=int, default=int(os.getenv("CHUNK_SIZE", "512")),
                        help=f'Chunk size for text splitting (default: {os.getenv("CHUNK_SIZE", "512")})')
    parser.add_argument('--chunk-overlap', type=int, default=int(os.getenv("CHUNK_OVERLAP", "50")),
                        help=f'Chunk overlap for text splitting (default: {os.getenv("CHUNK_OVERLAP", "50")})')
    parser.add_argument('--embed-batch-size', type=int, default=int(os.getenv("API_BATCH_SIZE", "32")),
                        help=f'Batch size for embedding generation (default: {os.getenv("API_BATCH_SIZE", "32")})')
    
    # Model configuration
    parser.add_argument('--embed-model', type=str, default=os.getenv("EMBED_MODEL", "text-embedding-3-small"),
                        help=f'OpenAI embedding model fallback (default: {os.getenv("EMBED_MODEL", "text-embedding-3-small")})')
    parser.add_argument('--llm-model', type=str, default=os.getenv("LLM_MODEL", "gpt-4o-mini"),
                        help=f'LLM model for summaries (default: {os.getenv("LLM_MODEL", "gpt-4o-mini")})')
                        
    # Output table configuration (legacy - enhanced pipeline uses standard tables)
    parser.add_argument('--embeddings-table', type=str, default=os.getenv("EMBEDDINGS_TABLE", "iland_embeddings"),
                        help=f'Legacy embeddings table name (default: {os.getenv("EMBEDDINGS_TABLE", "iland_embeddings")})')
    
    # Processing control
    parser.add_argument('--skip-processing', action='store_true',
                        help='Skip data processing step (only generate embeddings)')
    parser.add_argument('--skip-embeddings', action='store_true',
                        help='Skip embedding generation step (only process data)')
    parser.add_argument('--no-province-filter', action='store_true',
                        help='Disable province filtering (process all provinces)')
    
    # Enhanced pipeline options
    parser.add_argument('--disable-section-chunking', action='store_true',
                        help='Disable section-based chunking (use sentence splitting instead)')
    parser.add_argument('--disable-multi-model', action='store_true',
                        help='Disable OpenAI fallback (BGE-only processing)')
    parser.add_argument('--min-section-size', type=int, default=50,
                        help='Minimum section size for chunking (default: 50)')
    
    # Security and compliance options
    parser.add_argument('--verify-local-only', action='store_true', default=False,
                        help='Verify that no external API calls are made (deprecated - use --disable-multi-model)')
    
    args = parser.parse_args()
    
    # If no-province-filter flag is set, set filter_province to None
    if args.no_province_filter:
        args.filter_province = None
    
    # Print header with PRD compliance information
    logger.info("=" * 80)
    logger.info("iLAND BGE-M3 RAG PIPELINE: PRD v2.0 IMPLEMENTATION")
    logger.info("Section-Based Chunking + pgVector Storage")
    logger.info("=" * 80)
    logger.info("ðŸ”’ SECURITY: BGE-M3 local processing with OpenAI fallback")
    logger.info("ðŸ“ CHUNKING: Section-based parsing (~6 chunks vs ~169)")
    logger.info("ðŸ¤– MODEL: BGE-M3 multilingual for Thai + OpenAI fallback")
    logger.info("ðŸ—„ï¸ STORAGE: pgVector single table structure (maintained)")
    logger.info("ðŸ“Š COMPLIANCE: PRD v2.0 with existing database structure")
    logger.info("=" * 80)
    
    # Validate critical environment variables
    if not args.disable_multi_model and not os.getenv("OPENAI_API_KEY"):
        logger.warning("âš ï¸ OPENAI_API_KEY not found - fallback may not work")
        logger.warning("   Consider using --disable-multi-model for BGE-only processing")
    
    logger.info(f"ðŸ”§ Configuration:")
    logger.info(f"   - Section chunking: {'Enabled' if not args.disable_section_chunking else 'Disabled'}")
    logger.info(f"   - OpenAI fallback: {'Enabled' if not args.disable_multi_model else 'Disabled'}")  
    logger.info(f"   - Min section size: {args.min_section_size}")
    logger.info(f"   - Chunk size: {args.chunk_size}, overlap: {args.chunk_overlap}")
    logger.info(f"   - Embed batch size: {args.embed_batch_size}")
    logger.info(f"   - pgVector table: {args.embeddings_table}")
    logger.info("=" * 80)
    
    start_time = time.time()
    
    try:
        # Step 1: Process data (unless skipped)
        document_count = 0
        if not args.skip_processing:
            document_count = process_data(args)
        else:
            logger.info("Skipping data processing step as requested")
            # If we're skipping processing but doing embeddings, use max_rows as document count
            document_count = args.max_rows or 1000
        
        # Step 2: Generate embeddings with BGE-M3 and section-based chunking
        if not args.skip_embeddings:                
            success = generate_embeddings(args, document_count)
            if not success:
                return 1
        else:
            logger.info("Skipping embedding generation step as requested")
        
        # Print completion message
        total_time = time.time() - start_time
        logger.info("\n" + "=" * 80)
        logger.info(f"ðŸŽ‰ PIPELINE COMPLETED SUCCESSFULLY in {total_time:.2f} seconds")
        logger.info("âœ… PRD v2.0 Requirements SATISFIED:")
        logger.info("   - BGE-M3 section-based chunking implementation")
        logger.info("   - pgVector storage structure maintained (no database changes)")
        logger.info("   - Section-based chunking reduces chunk count dramatically")
        logger.info("   - Complete metadata preservation and enhancement")
        logger.info("   - BGE-M3 local processing with OpenAI fallback")
        logger.info("   - Production-ready with comprehensive error handling")
        logger.info("=" * 80)
        
        return 0
        
    except Exception as e:
        logger.error(f"Error during pipeline execution: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main()) 