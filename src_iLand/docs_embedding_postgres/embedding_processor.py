"""
Enhanced BGE-M3 Embedding Processor for PostgreSQL with Section-Based Chunking

This module implements the PRD v2.0 specifications:
- BGE-M3 model for 100% local processing (no external API calls)
- Section-based chunking reducing chunks from ~169 to ~6 per document
- Complete metadata preservation with security compliance
- Zero external data transmission for government deployment

Author: AI Assistant (PRD v2.0 Implementation)
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
from dotenv import load_dotenv

# Import LlamaIndex for document processing
from llama_index.core import Document, Settings
from llama_index.core.schema import TextNode

# Import BGE model support
try:
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    BGE_AVAILABLE = True
except ImportError:
    BGE_AVAILABLE = False
    print("‚ö†Ô∏è BGE embeddings not available. Install with: pip install llama-index-embeddings-huggingface sentence-transformers")

# Import from internal modules
try:
    from .standalone_section_parser import StandaloneLandDeedSectionParser
    from .db_utils import PostgresManager
except ImportError:
    from standalone_section_parser import StandaloneLandDeedSectionParser
    from db_utils import PostgresManager

# Load environment variables
load_dotenv()

# Configure logging
logger = logging.getLogger(__name__)


class BGEEmbeddingProcessor:
    """
    Enhanced BGE-M3 Embedding Processor with Section-Based Chunking
    
    Implements PRD v2.0 specifications for iLand RAG Pipeline:
    - Uses BGE-M3 model for multilingual Thai support
    - Section-based chunking for efficient document representation
    - 100% local processing with no external API calls
    - Enhanced metadata preservation for government compliance
    """
    
    # BGE-M3 Model Configurations (as per PRD)
    BGE_MODELS = {
        "bge-m3": {
            "model_name": "BAAI/bge-m3",
            "dimension": 1024,
            "max_length": 8192,
            "description": "Multilingual BGE model with Thai support",
            "recommended": True
        },
        "bge-large-en-v1.5": {
            "model_name": "BAAI/bge-large-en-v1.5",
            "dimension": 1024,
            "max_length": 512,
            "description": "High-quality English BGE model",
            "recommended": False
        },
        "bge-base-en-v1.5": {
            "model_name": "BAAI/bge-base-en-v1.5", 
            "dimension": 768,
            "max_length": 512,
            "description": "Balanced BGE model",
            "recommended": False
        }
    }
    
    def __init__(
        self, 
        # BGE Model Configuration (PRD v2.0 defaults to BGE-M3)
        bge_model_key: str = "bge-m3",
        model_cache_dir: str = os.getenv("BGE_CACHE_FOLDER", "./cache/bge_models"),
        device: str = "auto",  # auto, cuda, cpu
        use_fp16: bool = True,
        normalize_embeddings: bool = True,
        
        # Section-Based Chunking Configuration
        chunk_size: int = int(os.getenv("CHUNK_SIZE", "512")),
        chunk_overlap: int = int(os.getenv("CHUNK_OVERLAP", "50")),
        min_section_size: int = 50,
        enable_section_chunking: bool = True,
        
        # Processing Configuration
        batch_size: int = int(os.getenv("BATCH_SIZE", "32")),
        max_length: int = 8192,
        
        # Security Configuration (PRD v2.0)
        allow_external_apis: bool = False,  # Enforce local-only processing
        audit_logging: bool = True
    ):
        """
        Initialize BGE-M3 Embedding Processor with Section-Based Chunking
        
        Args:
            bge_model_key: BGE model to use (default: bge-m3 for Thai support)
            model_cache_dir: Directory to cache BGE models
            device: Device to use (auto, cuda, cpu)
            use_fp16: Use half precision for efficiency
            normalize_embeddings: Normalize embedding vectors
            chunk_size: Maximum chunk size in tokens
            chunk_overlap: Overlap between chunks
            min_section_size: Minimum size for standalone sections
            enable_section_chunking: Enable section-based parsing
            batch_size: Batch size for embedding generation
            max_length: Maximum text length for processing
            allow_external_apis: Allow external API calls (DISABLED by default)
            audit_logging: Enable audit logging for compliance
        """
        # Model Configuration
        self.bge_model_key = bge_model_key
        self.model_cache_dir = Path(model_cache_dir)
        self.device = device
        self.use_fp16 = use_fp16
        self.normalize_embeddings = normalize_embeddings
        
        # Processing Configuration
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_section_size = min_section_size
        self.enable_section_chunking = enable_section_chunking
        self.batch_size = batch_size
        self.max_length = max_length
        
        # Security Configuration (PRD v2.0)
        self.allow_external_apis = allow_external_apis
        self.audit_logging = audit_logging
        
        # Validate BGE model
        if self.bge_model_key not in self.BGE_MODELS:
            raise ValueError(f"Unsupported BGE model: {self.bge_model_key}. Supported: {list(self.BGE_MODELS.keys())}")
        
        self.model_config = self.BGE_MODELS[self.bge_model_key]
        
        # Initialize components
        self.embed_model = None
        self.section_parser = None
        self.processing_stats = {
            "documents_processed": 0,
            "total_chunks": 0,
            "section_chunks": 0,
            "fallback_chunks": 0,
            "avg_chunks_per_doc": 0,
            "metadata_fields_preserved": 0,
            "embedding_model": self.bge_model_key,
            "embedding_dimension": self.model_config["dimension"],
            "local_processing_only": not self.allow_external_apis,
            "section_chunking_enabled": self.enable_section_chunking
        }
        
        # Initialize BGE model and section parser
        self._initialize_bge_model()
        self._initialize_section_parser()
        
        # Audit log initialization
        if self.audit_logging:
            self._audit_log("PROCESSOR_INIT", {
                "model": self.bge_model_key,
                "dimension": self.model_config["dimension"],
                "local_only": not self.allow_external_apis,
                "section_chunking": self.enable_section_chunking
            })
        
        logger.info(f"ü§ó BGE-M3 Embedding Processor initialized")
        logger.info(f"   Model: {self.bge_model_key} ({self.model_config['dimension']}d)")
        logger.info(f"   Section chunking: {'ENABLED' if self.enable_section_chunking else 'DISABLED'}")
        logger.info(f"   External APIs: {'DISABLED' if not self.allow_external_apis else 'ENABLED'}")
        logger.info(f"   Device: {self.device}")
    
    def _initialize_bge_model(self):
        """Initialize BGE-M3 model for local embeddings."""
        logger.info(f"Initializing BGE model: {self.bge_model_key}")
        logger.info(f"Device: {self.device}")
        logger.info(f"Model configuration: {self.model_config}")
        
        # Validate model configuration
        if self.bge_model_key not in self.BGE_MODELS:
            raise ValueError(f"Unsupported BGE model: {self.bge_model_key}")
        
        # Security audit - no external APIs
        if self.allow_external_apis:
            logger.warning("‚ö†Ô∏è External APIs are allowed - this may violate PRD v2.0 security requirements")
        else:
            logger.info("üîí External APIs disabled - PRD v2.0 security compliance")
        
        try:
            # Ensure model cache directory exists
            self.model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"Initializing BGE model: {self.model_config['model_name']}")
            logger.info(f"Cache directory: {self.model_cache_dir}")
            
            # Initialize BGE model
            self.embed_model = HuggingFaceEmbedding(
                model_name=self.model_config['model_name'],
                cache_folder=str(self.model_cache_dir),
                device=self.device,
                max_length=self.model_config['max_length'],
                normalize=self.normalize_embeddings
            )
            
            # Configure LlamaIndex to use our embedding model
            Settings.embed_model = self.embed_model
            
            logger.info(f"‚úÖ BGE model initialized successfully")
            logger.info(f"   Dimension: {self.model_config['dimension']}")
            logger.info(f"   Max length: {self.model_config['max_length']}")
                      
        except Exception as e:
            logger.error(f"Failed to initialize BGE model: {e}")
            raise RuntimeError(f"BGE model initialization failed: {e}")
    
    def _initialize_section_parser(self):
        """Initialize section-based parser for Thai land deeds."""
        if self.enable_section_chunking:
            self.section_parser = StandaloneLandDeedSectionParser(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                min_section_size=self.min_section_size
            )
            logger.info("‚úÖ Section-based parser initialized for Thai land deed documents")
        else:
            logger.info("‚ÑπÔ∏è Section-based chunking disabled - using standard sentence splitting")
    
    def _audit_log(self, event: str, data: Dict[str, Any]):
        """Log events for security audit compliance."""
        if self.audit_logging:
            audit_entry = {
                "timestamp": datetime.now().isoformat(),
                "event": event,
                "data": data,
                "processor_id": id(self)
            }
            logger.info(f"AUDIT: {audit_entry}")
    
    def get_model_dimension(self) -> int:
        """Get the dimension of the current BGE model."""
        return self.model_config["dimension"]
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get comprehensive model information."""
        return {
            "model_key": self.bge_model_key,
            "model_name": self.model_config["model_name"],
            "dimension": self.model_config["dimension"],
            "max_length": self.model_config["max_length"],
            "description": self.model_config["description"],
            "local_processing": not self.allow_external_apis,
            "section_chunking": self.enable_section_chunking,
            "device": self.device,
            "cache_dir": str(self.model_cache_dir)
        }
    
    def process_documents_to_nodes(self, documents: List[Dict[str, Any]]) -> List[TextNode]:
        """
        Process documents using section-based chunking to create TextNodes.
        
        This is the core method that implements PRD v2.0 section-based chunking,
        reducing chunks from ~169 to ~6 per document.
        
        Args:
            documents: List of document dictionaries with content and metadata
            
        Returns:
            List of TextNode objects with section-aware metadata
        """
        all_nodes = []
        
        logger.info(f"Processing {len(documents)} documents with section-based chunking")
        
        for doc_idx, doc_data in enumerate(documents):
            deed_id = doc_data.get("deed_id", f"doc_{doc_idx}")
            content = doc_data.get("content", "")
            metadata = doc_data.get("metadata", {})
            
            # Audit logging for document processing
            self._audit_log("DOC_PROCESS_START", {
                "deed_id": deed_id,
                "content_length": len(content),
                "metadata_fields": len(metadata)
            })
            
            try:
                if self.enable_section_chunking and self.section_parser:
                    # Use section-based chunking (PRD v2.0 specification)
                    document_nodes = self.section_parser.parse_document_to_sections(
                        document_text=content,
                        metadata=metadata
                    )
                    
                    # Enhance metadata for each node
                    for node in document_nodes:
                        node.metadata.update({
                            "processing_method": "section_based",
                            "embedding_model": self.bge_model_key,
                            "embedding_dimension": self.model_config["dimension"],
                            "processed_locally": True,
                            "external_apis_used": [],
                            "data_transmitted_externally": False,
                            "processing_timestamp": datetime.now().isoformat(),
                            "prd_version": "2.0",
                            "security_compliant": True
                        })
                    
                    # Update statistics
                    self.processing_stats["section_chunks"] += len(document_nodes)
                    
                else:
                    # Fallback to standard chunking (should be avoided in PRD v2.0)
                    logger.warning(f"Using fallback chunking for document {deed_id}")
                    
                    # Create a single fallback node
                    fallback_metadata = {
                        **metadata,
                        "processing_method": "fallback",
                        "chunk_type": "fallback",
                        "section": "full_document",
                        "embedding_model": self.bge_model_key,
                        "embedding_dimension": self.model_config["dimension"],
                        "processed_locally": True,
                        "external_apis_used": [],
                        "data_transmitted_externally": False,
                        "processing_timestamp": datetime.now().isoformat(),
                        "prd_version": "2.0",
                        "security_compliant": True
                    }
                    
                    document_nodes = [TextNode(
                        text=content[:self.chunk_size],  # Truncate if too long
                        metadata=fallback_metadata
                    )]
                    
                    self.processing_stats["fallback_chunks"] += len(document_nodes)
                
                # Add nodes to results
                all_nodes.extend(document_nodes)
                
                # Update processing statistics
                self.processing_stats["documents_processed"] += 1
                self.processing_stats["total_chunks"] += len(document_nodes)
                self.processing_stats["metadata_fields_preserved"] += len(metadata)
                
                # Audit logging for successful processing
                self._audit_log("DOC_PROCESS_SUCCESS", {
                    "deed_id": deed_id,
                    "chunks_created": len(document_nodes),
                    "processing_method": "section_based" if self.enable_section_chunking else "fallback"
                })
                
                # Log progress
                if (doc_idx + 1) % 10 == 0:
                    avg_chunks = self.processing_stats["total_chunks"] / self.processing_stats["documents_processed"]
                    logger.info(f"Processed {doc_idx + 1}/{len(documents)} documents (avg {avg_chunks:.1f} chunks/doc)")
                        
            except Exception as e:
                logger.error(f"Error processing document {deed_id}: {e}")
                
                # Audit log the error
                self._audit_log("DOC_PROCESS_ERROR", {
                    "deed_id": deed_id,
                    "error": str(e)
                })
                
                # Continue with next document
                continue
        
        # Calculate final statistics
        if self.processing_stats["documents_processed"] > 0:
            self.processing_stats["avg_chunks_per_doc"] = (
                self.processing_stats["total_chunks"] / self.processing_stats["documents_processed"]
            )
        
        # Log final processing results
        logger.info("üéâ Document processing completed!")
        logger.info(f"üìä PROCESSING SUMMARY:")
        logger.info(f"   - Documents processed: {self.processing_stats['documents_processed']}")
        logger.info(f"   - Total chunks created: {self.processing_stats['total_chunks']}")
        logger.info(f"   - Section chunks: {self.processing_stats['section_chunks']}")
        logger.info(f"   - Fallback chunks: {self.processing_stats['fallback_chunks']}")
        logger.info(f"   - Average chunks per document: {self.processing_stats['avg_chunks_per_doc']:.1f}")
        logger.info(f"   - Metadata fields preserved: {self.processing_stats['metadata_fields_preserved']}")
        
        # Verify PRD v2.0 compliance
        if self.processing_stats["avg_chunks_per_doc"] <= 15:
            logger.info("‚úÖ PRD v2.0 COMPLIANCE: Efficient chunking achieved (‚â§15 chunks per doc)")
        else:
            logger.warning(f"‚ö†Ô∏è PRD v2.0 WARNING: High chunk count ({self.processing_stats['avg_chunks_per_doc']:.1f} per doc)")
        
        if self.processing_stats["fallback_chunks"] == 0:
            logger.info("‚úÖ PRD v2.0 COMPLIANCE: All documents processed with section-based chunking")
        else:
            logger.warning(f"‚ö†Ô∏è PRD v2.0 WARNING: {self.processing_stats['fallback_chunks']} fallback chunks created")
        
        return all_nodes
    
    def generate_embeddings_batch(self, nodes: List[TextNode]) -> List[TextNode]:
        """
        Generate BGE-M3 embeddings for a batch of TextNodes.
        
        Args:
            nodes: List of TextNode objects to generate embeddings for
            
        Returns:
            List of TextNode objects with embeddings added
        """
        if not nodes:
            return []
        
        logger.info(f"Generating BGE-M3 embeddings for {len(nodes)} chunks")
        
        # Audit logging for embedding generation
        self._audit_log("EMBEDDING_GENERATION_START", {
            "node_count": len(nodes),
            "model": self.bge_model_key,
            "batch_size": self.batch_size
        })
        
        try:
            # Extract texts for embedding
            texts = [node.text for node in nodes]
            
            # Generate embeddings in batches to manage memory
            embedded_nodes = []
            for i in range(0, len(nodes), self.batch_size):
                batch_nodes = nodes[i:i + self.batch_size]
                batch_texts = texts[i:i + self.batch_size]
                
                # Generate embeddings using BGE model
                batch_embeddings = self.embed_model.get_text_embedding_batch(batch_texts)
                
                # Add embeddings to nodes
                for node, embedding in zip(batch_nodes, batch_embeddings):
                    # Verify embedding dimension
                    if len(embedding) != self.model_config["dimension"]:
                        raise ValueError(
                            f"Embedding dimension mismatch: expected {self.model_config['dimension']}, "
                            f"got {len(embedding)}"
                        )
                    
                    # Add embedding to node
                    node.embedding = embedding
                    
                    # Update metadata with embedding info
                    node.metadata.update({
                        "embedding_generated": True,
                        "embedding_model_used": self.bge_model_key,
                        "embedding_dimension_verified": len(embedding),
                        "embedding_generation_timestamp": datetime.now().isoformat()
                    })
                
                embedded_nodes.extend(batch_nodes)
                
                # Log progress
                if (i + self.batch_size) % (self.batch_size * 5) == 0:
                    logger.info(f"Generated embeddings for {len(embedded_nodes)}/{len(nodes)} chunks")
            
            # Audit logging for successful embedding generation
            self._audit_log("EMBEDDING_GENERATION_SUCCESS", {
                "node_count": len(embedded_nodes),
                "model": self.bge_model_key,
                "dimension": self.model_config["dimension"]
            })
            
            logger.info(f"‚úÖ Generated {len(embedded_nodes)} BGE-M3 embeddings")
            logger.info(f"   Model: {self.bge_model_key}")
            logger.info(f"   Dimension: {self.model_config['dimension']}")
            logger.info(f"   Local processing: {not self.allow_external_apis}")
            
            return embedded_nodes
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            
            # Audit log the error
            self._audit_log("EMBEDDING_GENERATION_ERROR", {
                "error": str(e),
                "node_count": len(nodes)
            })
            
            raise
    
    def process_and_embed_documents(self, documents: List[Dict[str, Any]]) -> List[TextNode]:
        """
        Complete pipeline: process documents to nodes and generate embeddings.
        
        This is the main method that implements the full PRD v2.0 pipeline.
        
        Args:
            documents: List of document dictionaries
            
        Returns:
            List of TextNode objects with embeddings and enhanced metadata
        """
        logger.info("üöÄ Starting BGE-M3 processing pipeline (PRD v2.0)")
        
        # Step 1: Process documents to nodes with section-based chunking
        nodes = self.process_documents_to_nodes(documents)
        
        if not nodes:
            logger.warning("No nodes created from documents")
            return []
        
        # Step 2: Generate BGE-M3 embeddings
        embedded_nodes = self.generate_embeddings_batch(nodes)
        
        # Final audit log
        self._audit_log("PIPELINE_COMPLETE", {
            "documents_input": len(documents),
            "nodes_output": len(embedded_nodes),
            "avg_chunks_per_doc": len(embedded_nodes) / max(len(documents), 1),
            "prd_compliance": len(embedded_nodes) / max(len(documents), 1) <= 15
        })
        
        logger.info("‚úÖ BGE-M3 processing pipeline completed successfully")
        
        return embedded_nodes
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """Get comprehensive processing statistics for monitoring and compliance."""
        return {
            **self.processing_stats,
            "model_info": self.get_model_info(),
            "compliance_status": {
                "prd_version": "2.0",
                "local_processing_only": not self.allow_external_apis,
                "section_chunking_enabled": self.enable_section_chunking,
                "efficient_chunking": self.processing_stats.get("avg_chunks_per_doc", 0) <= 15,
                "no_fallback_chunks": self.processing_stats.get("fallback_chunks", 0) == 0,
                "metadata_preserved": self.processing_stats.get("metadata_fields_preserved", 0) > 0
            }
        }
